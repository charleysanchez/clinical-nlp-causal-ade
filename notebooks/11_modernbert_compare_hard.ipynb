{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2512309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          TrainingArguments, Trainer)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, accuracy_score\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_PATH = os.environ.get(\"DATA_DIR\", \"../data/synth_clinical\")\n",
    "NOTES_HARD_CSV = os.path.join(DATA_PATH, \"notes_hard.csv\")\n",
    "LABELS_CSV = os.path.join(DATA_PATH, \"docs_labels_hard.csv\")\n",
    "\n",
    "notes = pd.read_csv(NOTES_HARD_CSV)\n",
    "labels = pd.read_csv(LABELS_CSV)\n",
    "df = notes.merge(labels[['doc_id', 'hard_label']], on='doc_id', how='left')\n",
    "df['label'] = df['hard_label'].astype(int)\n",
    "\n",
    "assert os.path.exists(NOTES_HARD_CSV) and os.path.exists(LABELS_CSV), f\"Missing notes.csv at {NOTES_HARD_CSV} or {LABELS_CSV}\"\n",
    "print(\"Using data:\", NOTES_HARD_CSV, LABELS_CSV)\n",
    "print(df['label'].value_counts())\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cd67e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=Dataset.from_pandas(df[['text','label']])\n",
    "ds=ds.train_test_split(test_size=0.2, seed=SEED)\n",
    "len(ds['train']), len(ds['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a85024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = logits - logits.max(axis=1, keepdims=True)\n",
    "    probs = np.exp(probs); probs = probs[:,1] / probs.sum(axis=1)\n",
    "    preds=(probs>=0.5).astype(int)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1': f1_score(labels, preds),\n",
    "        'auroc': roc_auc_score(labels, probs),\n",
    "        'auprc': average_precision_score(labels, probs)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec11d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_name:str, max_len=768, epochs=3, batch=16, lr=2e-5, fp16=True):\n",
    "    tok=AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "    enc=ds.map(lambda x: tok(x['text'], max_length=max_len, truncation=True), batched=True, remove_columns=['text'])\n",
    "    model=AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    args=TrainingArguments(\n",
    "        output_dir=f\"./reports/doc_cls_hard_{model_name.replace('/','_')}\",\n",
    "        per_device_train_batch_size=batch,\n",
    "        per_device_eval_batch_size=batch,\n",
    "        num_train_epochs=epochs,\n",
    "        learning_rate=lr,\n",
    "        fp16=fp16,\n",
    "        logging_steps=50,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='no',\n",
    "        report_to=[],\n",
    "        seed=SEED\n",
    "    )\n",
    "    trainer=Trainer(model=model, args=args, train_dataset=enc['train'], eval_dataset=enc['test'], tokenizer=tok, compute_metrics=compute_metrics)\n",
    "    import time; t0=time.time(); trainer.train(); dur=time.time()-t0\n",
    "    metrics=trainer.evaluate(); metrics['seconds']=dur; return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946d431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results={}\n",
    "for name, mn in [(\"BioClinical ModernBERT\", \"thomas-sounack/BioClinical-ModernBERT-base\"),\n",
    "                 (\"ModernBERT (vanilla)\", \"answerdotai/ModernBERT-base\")]:\n",
    "    print(f\"\\n==== Training {name}: {mn} ====\")\n",
    "    results[name]=run_model(mn)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).T[['accuracy','f1','auroc','auprc','eval_loss','seconds']]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
