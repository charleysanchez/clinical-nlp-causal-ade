{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90382b05",
   "metadata": {},
   "source": [
    "# BioClinical ModernBERT vs ModernBERT — Causal ADE Classification (Synthetic Notes)\n",
    "\n",
    "This notebook benchmarks **BioClinical ModernBERT** against **vanilla ModernBERT**\n",
    "for detecting **causal Adverse Drug Events (ADEs)** in synthetic ICU notes.\n",
    "\n",
    "We use a synthetic dataset (`notes_hard_v4.csv`, `doc_labels_hard_v4`) that contains both **textual** and **structural** signals \n",
    "for drug–ADE relationships. Each note is labeled positive only when both:\n",
    "\n",
    "- The patient was *treated with an ACE inhibitor* (`T=1`), **and**\n",
    "- The note explicitly links the treatment to an **adverse outcome** (`AKI=1` with causal phrasing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd08a4b5",
   "metadata": {},
   "source": [
    "## 1. Load the Synthetic Dataset\n",
    "\n",
    "We load `notes_hard.csv` and `doc_labels_hard.csv`, then merge them on `doc_id`.  \n",
    "Each note contains free text describing an ICU admission, while `hard_label` encodes the binary ADE outcome.\n",
    "\n",
    "The dataset has:\n",
    "- Text field (`text`)\n",
    "- Label field (`hard_label`)\n",
    "- Roughly balanced class distribution\n",
    "\n",
    "Let's preview and confirm the merge worked correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2512309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          TrainingArguments, Trainer, DataCollatorWithPadding)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, accuracy_score\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = os.environ.get(\"DATA_DIR\", \"../data/synth_clinical\")\n",
    "\n",
    "notes_v4  = os.path.join(DATA_DIR, \"notes_hard_v4.csv\")\n",
    "labels_v4 = os.path.join(DATA_DIR, \"doc_labels_hard_v4.csv\")\n",
    "\n",
    "notes = pd.read_csv(notes_v4)\n",
    "labs  = pd.read_csv(labels_v4)\n",
    "df = notes.merge(labs[['doc_id','label','split']], on='doc_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff9ab4e",
   "metadata": {},
   "source": [
    "## 2. Prepare the Dataset for Modeling\n",
    "\n",
    "We convert the merged dataframe into a Hugging Face `Dataset` and split 80/20 into \n",
    "training and test subsets.\n",
    "\n",
    "This allows us to evaluate generalization on unseen synthetic notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70bf5704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6464, 1536)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = df[df['split']=='train'][['text','label']].reset_index(drop=True)\n",
    "test_df  = df[df['split']=='test'][['text','label']].reset_index(drop=True)\n",
    "\n",
    "ds = {\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"test\":  Dataset.from_pandas(test_df),\n",
    "}\n",
    "len(ds[\"train\"]), len(ds[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce30f32e",
   "metadata": {},
   "source": [
    "## 3. Define Evaluation Metrics\n",
    "\n",
    "For consistency with biomedical NLP literature, we evaluate:\n",
    "\n",
    "- **Accuracy** — overall classification correctness  \n",
    "- **F1 Score** — harmonic mean of precision & recall  \n",
    "- **AUROC** — area under the ROC curve (ranking ability)  \n",
    "- **AUPRC** — area under the precision–recall curve (robust to imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16a85024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = logits - logits.max(axis=1, keepdims=True)\n",
    "    probs = np.exp(probs)\n",
    "    probs = probs[:,1] / probs.sum(axis=1)\n",
    "    preds=(probs>=0.5).astype(int)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1': f1_score(labels, preds),\n",
    "        'auroc': roc_auc_score(labels, probs),\n",
    "        'auprc': average_precision_score(labels, probs)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db4db10",
   "metadata": {},
   "source": [
    "## 4. Define Model Training Routine\n",
    "\n",
    "We create a helper `run_model()` that:\n",
    "\n",
    "1. Loads the chosen pretrained ModernBERT tokenizer & model  \n",
    "2. Tokenizes text up to a maximum sequence length  \n",
    "3. Trains for a configurable number of epochs  \n",
    "4. Logs key metrics on the validation set after each epoch\n",
    "\n",
    "The function returns a dictionary of performance statistics for easy comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec11d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_name:str, max_len=768, epochs=3, batch=16, lr=2e-5, fp16=True):\n",
    "    tok = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "\n",
    "    def tok_fn(b):\n",
    "        return tok(b[\"text\"], max_length=max_len, truncation=True)\n",
    "\n",
    "    enc = {k: v.map(tok_fn, batched=True, remove_columns=[\"text\"]) for k, v in ds.items()}\n",
    "    data_collator = DataCollatorWithPadding(tok)  # dynamic padding per batch\n",
    "    \n",
    "    model=AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    args=TrainingArguments(\n",
    "        output_dir=f\"../reports/doc_cls_hard_{model_name.replace('/','_')}\",\n",
    "        per_device_train_batch_size=batch,\n",
    "        per_device_eval_batch_size=batch,\n",
    "        num_train_epochs=epochs,\n",
    "        learning_rate=lr,\n",
    "        weight_decay=0.1,\n",
    "        lr_scheduler_type='cosine',\n",
    "        warmup_ratio=0.1,\n",
    "        fp16=fp16,\n",
    "        logging_steps=50,\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        label_smoothing_factor=0.2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_auprc\",\n",
    "        report_to=[],\n",
    "        max_grad_norm=1.0,\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=2,\n",
    "        seed=SEED\n",
    "    )\n",
    "    trainer=Trainer(model=model, args=args, train_dataset=enc['train'], eval_dataset=enc['test'], processing_class=tok, \n",
    "                    data_collator=data_collator, compute_metrics=compute_metrics)\n",
    "    import time; t0=time.time(); trainer.train(); dur=time.time()-t0\n",
    "    metrics=trainer.evaluate(); metrics['seconds']=dur; return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162111a8",
   "metadata": {},
   "source": [
    "## 5. Train and Compare Models\n",
    "\n",
    "We benchmark two architectures:\n",
    "\n",
    "| Model | Description |\n",
    "|--------|--------------|\n",
    "| **BioClinical ModernBERT** | Domain-adapted version trained on biomedical corpora (MIMIC, PubMed) |\n",
    "| **ModernBERT (vanilla)** | General English version trained on large web text |\n",
    "\n",
    "Both are fine-tuned for binary sequence classification on our synthetic ADE dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "946d431a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Training BioClinical ModernBERT: thomas-sounack/BioClinical-ModernBERT-base ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6464/6464 [00:00<00:00, 28459.27 examples/s]\n",
      "Map: 100%|██████████| 1536/1536 [00:00<00:00, 25080.10 examples/s]\n",
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at thomas-sounack/BioClinical-ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1212' max='1212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1212/1212 02:52, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Auroc</th>\n",
       "      <th>Auprc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.458400</td>\n",
       "      <td>0.425841</td>\n",
       "      <td>0.947266</td>\n",
       "      <td>0.948831</td>\n",
       "      <td>0.950244</td>\n",
       "      <td>0.942866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.419700</td>\n",
       "      <td>0.418756</td>\n",
       "      <td>0.954427</td>\n",
       "      <td>0.955471</td>\n",
       "      <td>0.952118</td>\n",
       "      <td>0.947214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.424200</td>\n",
       "      <td>0.408924</td>\n",
       "      <td>0.954427</td>\n",
       "      <td>0.955471</td>\n",
       "      <td>0.952524</td>\n",
       "      <td>0.951783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.386900</td>\n",
       "      <td>0.410186</td>\n",
       "      <td>0.954427</td>\n",
       "      <td>0.955471</td>\n",
       "      <td>0.949895</td>\n",
       "      <td>0.951002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.408628</td>\n",
       "      <td>0.954427</td>\n",
       "      <td>0.955471</td>\n",
       "      <td>0.951059</td>\n",
       "      <td>0.947857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.380300</td>\n",
       "      <td>0.408561</td>\n",
       "      <td>0.954427</td>\n",
       "      <td>0.955471</td>\n",
       "      <td>0.951160</td>\n",
       "      <td>0.949261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Training ModernBERT (vanilla): answerdotai/ModernBERT-base ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6464/6464 [00:00<00:00, 28376.67 examples/s]\n",
      "Map: 100%|██████████| 1536/1536 [00:00<00:00, 29342.55 examples/s]\n",
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1212' max='1212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1212/1212 02:50, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Auroc</th>\n",
       "      <th>Auprc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.464200</td>\n",
       "      <td>0.438118</td>\n",
       "      <td>0.933594</td>\n",
       "      <td>0.936803</td>\n",
       "      <td>0.954800</td>\n",
       "      <td>0.951175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.409000</td>\n",
       "      <td>0.408706</td>\n",
       "      <td>0.954427</td>\n",
       "      <td>0.955471</td>\n",
       "      <td>0.954127</td>\n",
       "      <td>0.955611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.414500</td>\n",
       "      <td>0.405766</td>\n",
       "      <td>0.954427</td>\n",
       "      <td>0.955471</td>\n",
       "      <td>0.954640</td>\n",
       "      <td>0.947457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.387400</td>\n",
       "      <td>0.405943</td>\n",
       "      <td>0.954427</td>\n",
       "      <td>0.955471</td>\n",
       "      <td>0.951401</td>\n",
       "      <td>0.950117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.389100</td>\n",
       "      <td>0.405240</td>\n",
       "      <td>0.954427</td>\n",
       "      <td>0.955471</td>\n",
       "      <td>0.953182</td>\n",
       "      <td>0.952365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.405845</td>\n",
       "      <td>0.954427</td>\n",
       "      <td>0.955471</td>\n",
       "      <td>0.953383</td>\n",
       "      <td>0.952728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'BioClinical ModernBERT': {'eval_loss': 0.4089237153530121,\n",
       "  'eval_accuracy': 0.9544270833333334,\n",
       "  'eval_f1': 0.955470737913486,\n",
       "  'eval_auroc': 0.9525244465571099,\n",
       "  'eval_auprc': 0.9517825891984698,\n",
       "  'eval_runtime': 1.8817,\n",
       "  'eval_samples_per_second': 816.263,\n",
       "  'eval_steps_per_second': 25.508,\n",
       "  'epoch': 6.0,\n",
       "  'seconds': 173.26331639289856},\n",
       " 'ModernBERT (vanilla)': {'eval_loss': 0.4087055027484894,\n",
       "  'eval_accuracy': 0.9544270833333334,\n",
       "  'eval_f1': 0.955470737913486,\n",
       "  'eval_auroc': 0.9541270541898683,\n",
       "  'eval_auprc': 0.9556107462964034,\n",
       "  'eval_runtime': 2.0301,\n",
       "  'eval_samples_per_second': 756.62,\n",
       "  'eval_steps_per_second': 23.644,\n",
       "  'epoch': 6.0,\n",
       "  'seconds': 171.17205333709717}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results={}\n",
    "for name, mn in [(\"BioClinical ModernBERT\", \"thomas-sounack/BioClinical-ModernBERT-base\"),\n",
    "                 (\"ModernBERT (vanilla)\", \"answerdotai/ModernBERT-base\")]:\n",
    "    print(f\"\\n==== Training {name}: {mn} ====\")\n",
    "    results[name]=run_model(mn, max_len=128, epochs=6, batch=32, lr=1e-5)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7b65c2",
   "metadata": {},
   "source": [
    "## 6. Results Summary\n",
    "\n",
    "Below we summarize final evaluation metrics after fine-tuning.\n",
    "\n",
    "Values close to **1.0** indicate that the task is relatively easy for these models — likely because \n",
    "the dataset contains strong lexical cues (\"after starting\", \"denies\", \"no evidence of\", etc.) \n",
    "that the models can exploit directly.\n",
    "\n",
    "Subsequent versions of the dataset (`hard_v2`, `hard_v3`, `hard_v4`) progressively remove such shortcuts \n",
    "to test deeper reasoning and contextual understanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7dbd453a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_auroc</th>\n",
       "      <th>eval_auprc</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BioClinical ModernBERT</th>\n",
       "      <td>0.954427</td>\n",
       "      <td>0.955471</td>\n",
       "      <td>0.952524</td>\n",
       "      <td>0.951783</td>\n",
       "      <td>0.408924</td>\n",
       "      <td>173.263316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ModernBERT (vanilla)</th>\n",
       "      <td>0.954427</td>\n",
       "      <td>0.955471</td>\n",
       "      <td>0.954127</td>\n",
       "      <td>0.955611</td>\n",
       "      <td>0.408706</td>\n",
       "      <td>171.172053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        eval_accuracy   eval_f1  eval_auroc  eval_auprc  \\\n",
       "BioClinical ModernBERT       0.954427  0.955471    0.952524    0.951783   \n",
       "ModernBERT (vanilla)         0.954427  0.955471    0.954127    0.955611   \n",
       "\n",
       "                        eval_loss     seconds  \n",
       "BioClinical ModernBERT   0.408924  173.263316  \n",
       "ModernBERT (vanilla)     0.408706  171.172053  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).T[['eval_accuracy','eval_f1','eval_auroc','eval_auprc','eval_loss','seconds']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f99e9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split  label\n",
      "test   1         796\n",
      "       0         740\n",
      "train  0        3235\n",
      "       1        3229\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.groupby('split')['label'].value_counts())#(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c53956",
   "metadata": {},
   "source": [
    "## 7. Simple Keyword Baseline Metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf300d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train keyword baseline accuracy: 0.595\n",
      "test keyword baseline accuracy: 0.540\n"
     ]
    }
   ],
   "source": [
    "CUES  = [\"after starting\", \"following initiation\", \"soon after\",\n",
    "         \"temporal association\", \"shortly post-initiation\", \"in close proximity\"]\n",
    "NEGS  = [\"no evidence of\", \"denies\", \"without signs of\", \"not \", \"unlikely to\"]\n",
    "\n",
    "def keyword_baseline(text):\n",
    "    t = text.lower()\n",
    "    cue = any(c in t for c in CUES)\n",
    "    neg = any(n in t for n in NEGS)\n",
    "    return int(cue and not neg)   # 1 = positive guess, else 0\n",
    "\n",
    "for split in [\"train\",\"test\"]:\n",
    "    p = df[df.split==split]\n",
    "    preds = p.text.map(keyword_baseline).values\n",
    "    acc = (preds == p.label.values).mean()\n",
    "    print(f\"{split} keyword baseline accuracy: {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e31820a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p90 length: 83.0\n"
     ]
    }
   ],
   "source": [
    "assert set(df[df.split=='train'].subject_id) & set(df[df.split=='test'].subject_id) == set(), \"Subject leakage!\"\n",
    "\n",
    "# Inspect truncation — if most notes are short, try smaller max_len (less capacity = less overfit)\n",
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"thomas-sounack/BioClinical-ModernBERT-base\", add_prefix_space=True)\n",
    "lens = df['text'].sample(200).apply(lambda s: len(tok(s, truncation=True)['input_ids']))\n",
    "print(\"p90 length:\", np.percentile(lens, 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a3cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
